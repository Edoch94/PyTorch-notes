{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient calculation with autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a simple tensor, and set its `requires_grad` parameter to `True`: in this way, if inserted in a function, the gradient of the function is going to be computed on this variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4051, 0.5039, 1.2624], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `y` variable that depends on `x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.4051, 2.5039, 3.2624], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the `grad_fn` attribute\n",
    "> Each variable has a `.grad_fn` attribute that references a function that has created a function (except for Tensors created by the user - these have `None` as `.grad_fn`) - [source](https://pytorch.org/tutorials/beginner/former_torchies/autograd_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `z` variable that depends on `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15.1317, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = (y**2*2).mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the gradient of `z` with respect to `x`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward() #dz/dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the gradient of `z` with respect to `x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.2068, 3.3386, 4.3499])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `z` must be a scalar value to implicitly compute the gradient. If you want to compute the gradient of a vector, the values of vector $v$ have to be specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([11.5690, 12.5394, 21.2867], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z2 = y**2*2\n",
    "print(z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([12.8272, 13.3544, 17.3996])\n"
     ]
    }
   ],
   "source": [
    "z2.backward(torch.Tensor([1.0,1.0,1.0]))\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, in the background `.backword()` computes a Jacobian-vector ($J*v$) product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop gradient computation and tracking history on computation graph\n",
    "It might be useful, in some cases, to exclude some operations form gradient computations. To do so, you can use:\n",
    "- `x.requires_grad_(False)` : this method sets `requires_grad` of `x` to `False` (notice that the trailing underscore of the function means that the tensor is going to be modified inplace)\n",
    "- `x.detach()` : this method creates a tensor with `requires_grad` set to `False`; however, notice that the new tensor shares the storage space with the old tensor, so any computation on the old tensor also updates the new tensor\n",
    "- `with torch.no_grad():` : this is a context manager, and operations made within it will be performed with tensors having `requires_grad` temporarily set to `False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.5404,  1.3869, -1.6816])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "x.requires_grad_(False)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.5733,  0.7729,  1.8764])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "x_new = x.detach()\n",
    "print(x_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-5.6967e-01, -5.9780e-04, -3.7614e+00])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "with torch.no_grad():\n",
    "    y = x*2\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient accumulation using `.backword()`\n",
    "Gradients should be re-initialized when computation involves loops, since PyTorch automatically accumulates gradients when `.backword()` is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([6., 6., 6., 6.])\n",
      "tensor([9., 9., 9., 9.])\n"
     ]
    }
   ],
   "source": [
    "# initialize x\n",
    "x = torch.ones(4, requires_grad=True)\n",
    "\n",
    "# create a loop: at each loop look at the value of x.grad\n",
    "for epoch in range(3):\n",
    "\n",
    "    # define a variable y that depends on x\n",
    "    y = (x*3).sum()\n",
    "\n",
    "    # compute the gradients wrt x\n",
    "    y.backward()\n",
    "\n",
    "    # print the gradient computed wrt x\n",
    "    print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradients are not correct anymore after the first iteration. To correct this problem use `.grad.zero_()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "# initialize x\n",
    "x = torch.ones(4, requires_grad=True)\n",
    "\n",
    "# create a loop: at each loop look at the value of x.grad\n",
    "for epoch in range(3):\n",
    "\n",
    "    # define a variable y that depends on x\n",
    "    y = (x*3).sum()\n",
    "\n",
    "    # compute the gradients wrt x\n",
    "    y.backward()\n",
    "\n",
    "    # print the gradient computed wrt x\n",
    "    print(x.grad)\n",
    "\n",
    "    # reset the gradients\n",
    "    x.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is particularly useful when optimizing the weights (i.e., the parameters with respect to which the gradient is computed on the loss function) of a NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([1., 1., 1., 1.], requires_grad=True) \n",
      "\n",
      "epoch 1\n",
      "x.grad: tensor([3., 3., 3., 3.])\n",
      "x_new[i] = x_old[i] - (x.grad[i] * lr) = 1.0 - (3.0 * 0.01) = 0.9700000286102295\n",
      "x: tensor([0.9700, 0.9700, 0.9700, 0.9700], requires_grad=True) \n",
      "\n",
      "epoch 2\n",
      "x.grad: tensor([3., 3., 3., 3.])\n",
      "x_new[i] = x_old[i] - (x.grad[i] * lr) = 0.9700000286102295 - (3.0 * 0.01) = 0.940000057220459\n",
      "x: tensor([0.9400, 0.9400, 0.9400, 0.9400], requires_grad=True) \n",
      "\n",
      "epoch 3\n",
      "x.grad: tensor([3., 3., 3., 3.])\n",
      "x_new[i] = x_old[i] - (x.grad[i] * lr) = 0.940000057220459 - (3.0 * 0.01) = 0.9100000858306885\n",
      "x: tensor([0.9100, 0.9100, 0.9100, 0.9100], requires_grad=True) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize x\n",
    "x = torch.ones(4, requires_grad=True)\n",
    "print(f'x: {x}', '\\n')\n",
    "\n",
    "# initialize the optimizer, defining the parameters to optimize and setting the learning rate\n",
    "params, lr = [x], 0.01 # the params parameter of SGD accepts only a list of parameters\n",
    "optimizer = torch.optim.SGD(params, lr)\n",
    "\n",
    "# create a loop: at each loop look at the value of x.grad\n",
    "for epoch in range(3):\n",
    "    print(f'epoch {epoch+1}')\n",
    "\n",
    "    # define a variable y that depends on x\n",
    "    y = (x*3).sum()\n",
    "\n",
    "    # compute the gradients wrt x\n",
    "    y.backward()\n",
    "\n",
    "    # print the gradient computed wrt x\n",
    "    print(f'x.grad: {x.grad}')\n",
    "    x_old = x.clone()\n",
    "\n",
    "    # launch an optimizer step: it updates the values of x to minimize the gradient\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f'x_new[i] = x_old[i] - (x.grad[i] * lr) = {x_old[0]} - ({x.grad[0]} * {lr}) = {x_old[0] - (x.grad[0] * lr)}')\n",
    "    print(f'x: {x}', '\\n')\n",
    "\n",
    "    # reset the gradients\n",
    "    x.grad.zero_()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('torch_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d94a055596b021af6b355d38c8f68b28eca02e4c507e6e85fc92b8f40d87732"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
