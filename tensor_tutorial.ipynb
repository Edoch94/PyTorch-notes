{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if PyTorch can use the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is GPU detected?\t\tTrue\n",
      "How many GPUs are detected?\t1\n",
      "GPU's name in use\t\tNVIDIA GeForce RTX 2070 with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "print(f\"Is GPU detected?\\t\\t{torch.cuda.is_available()}\")\n",
    "print(f\"How many GPUs are detected?\\t{torch.cuda.device_count()}\")\n",
    "print(f\"GPU's name in use\\t\\t{torch.cuda.get_device_name(torch.cuda.current_device())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set the default for new tensors to use the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.set_default_tensor_type(torch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set a variable that allows to move tensors to the GPU (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "torch_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(torch_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To move a tensor to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device used by rand_tensor:\tcpu\n",
      "device used by rand_tensor:\tcuda:0\n"
     ]
    }
   ],
   "source": [
    "rand_tensor = torch.rand(100)\n",
    "print(f\"device used by rand_tensor:\\t{rand_tensor.device}\")\n",
    "rand_tensor = rand_tensor.to(device=torch_device)\n",
    "print(f\"device used by rand_tensor:\\t{rand_tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a tensor directly in the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device used by rand_tensor:\tcuda:0\n"
     ]
    }
   ],
   "source": [
    "rand_tensor = torch.rand(1000000, device=torch_device)\n",
    "print(f\"device used by rand_tensor:\\t{rand_tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000256"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the GPU usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2070 with Max-Q Design\n",
      "Memory Usage:\n",
      "Allocated (in GB):\t 0.004\n",
      "Cached (in GB):\t\t 0.021\n",
      "Allocated (in byte):\t 4000256\n",
      "Cached (in byte):\t 23068672\n",
      "Maximum GPU memory occupied by tensors in bytes:\t\t4040704\n",
      "Maximum GPU memory managed by the caching allocator in bytes:\t23068672\n"
     ]
    }
   ],
   "source": [
    "if torch_device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated (in GB):\\t', round(torch.cuda.memory_allocated(torch.cuda.current_device())/1024**3,3))\n",
    "    print('Cached (in GB):\\t\\t', round(torch.cuda.memory_reserved(torch.cuda.current_device())/1024**3,3))\n",
    "    print('Allocated (in byte):\\t', torch.cuda.memory_allocated(torch.cuda.current_device()))\n",
    "    print('Cached (in byte):\\t', torch.cuda.memory_reserved(torch.cuda.current_device()))\n",
    "    print(f'Maximum GPU memory occupied by tensors in bytes:\\t\\t{torch.cuda.max_memory_allocated()}')\n",
    "    print(f'Maximum GPU memory managed by the caching allocator in bytes:\\t{torch.cuda.max_memory_reserved()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rand_tensor:\n",
      "tensor([[0.8494, 0.2287, 0.3451],\n",
      "        [0.7794, 0.1065, 0.6900]])\n",
      "\n",
      "ones_tensor:\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "\n",
      "zeros_tensor:\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "full_tensor:\n",
      "tensor([[2, 2, 2],\n",
      "        [2, 2, 2]])\n",
      "\n",
      "tensor_from_numpy_array:\n",
      "tensor([[0.5753, 0.1425, 0.4476],\n",
      "        [0.1436, 0.2854, 0.3762]], dtype=torch.float64)\n",
      "\n",
      "tensor_from_list:\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rand_tensor = torch.rand(2,3)\n",
    "ones_tensor = torch.ones(2,3)\n",
    "zeros_tensor = torch.zeros(2,3)\n",
    "full_tensor = torch.full((2,3),2)\n",
    "tensor_from_numpy_array = torch.from_numpy(np.random.rand(2,3))\n",
    "tensor_from_list = torch.tensor([[1,2,3],[4,5,6]])\n",
    "\n",
    "\n",
    "print(f\"rand_tensor:\\n{rand_tensor}\\n\")\n",
    "print(f\"ones_tensor:\\n{ones_tensor}\\n\")\n",
    "print(f\"zeros_tensor:\\n{zeros_tensor}\\n\")\n",
    "print(f\"full_tensor:\\n{full_tensor}\\n\")\n",
    "print(f\"tensor_from_numpy_array:\\n{tensor_from_numpy_array}\\n\")\n",
    "print(f\"tensor_from_list:\\n{tensor_from_list}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rand_tensor shape:\ttorch.Size([2, 3])\n",
      "rand_tensor size:\ttorch.Size([2, 3])\n",
      "rand_tensor dtype:\ttorch.float32\n",
      "rand_tensor device:\tcpu\n"
     ]
    }
   ],
   "source": [
    "print(f'rand_tensor shape:\\t{rand_tensor.shape}')\n",
    "print(f'rand_tensor size:\\t{rand_tensor.size()}')\n",
    "print(f'rand_tensor dtype:\\t{rand_tensor.dtype}')\n",
    "print(f'rand_tensor device:\\t{rand_tensor.device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concat_tensor:\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.]], device='cuda:0')\n",
      "\n",
      "concat_tensor_dim_1:\n",
      "tensor([[1., 1., 1., 2., 2., 2.],\n",
      "        [1., 1., 1., 2., 2., 2.]], device='cuda:0')\n",
      "\n",
      "stack_tensor:\n",
      "tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[2., 2., 2.],\n",
      "         [2., 2., 2.]]], device='cuda:0')\n",
      "\n",
      "stack_tensor_dim_1:\n",
      "tensor([[[1., 1., 1.],\n",
      "         [2., 2., 2.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [2., 2., 2.]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "ones_tensor = torch.ones(2,3).to(torch_device)\n",
    "twos_tensor = torch.full((2,3),2).to(torch_device)\n",
    "\n",
    "concat_tensor = torch.cat([ones_tensor, twos_tensor])\n",
    "concat_tensor_dim_1 = torch.cat([ones_tensor, twos_tensor], dim=1)\n",
    "stack_tensor = torch.stack([ones_tensor, twos_tensor])\n",
    "stack_tensor_dim_1 = torch.stack([ones_tensor, twos_tensor],dim=1)\n",
    "\n",
    "print(f'concat_tensor:\\n{concat_tensor}\\n')\n",
    "print(f'concat_tensor_dim_1:\\n{concat_tensor_dim_1}\\n')\n",
    "print(f'stack_tensor:\\n{stack_tensor}\\n')\n",
    "print(f'stack_tensor_dim_1:\\n{stack_tensor_dim_1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slicing and indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [2., 2.]], device='cuda:0')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_tensor[0::2,[0,2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplycation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twos_tensor * fours_tensor:\n",
      "tensor([[8, 8, 8],\n",
      "        [8, 8, 8]], device='cuda:0')\n",
      "\n",
      "twos_tensor.mul(fours_tensor):\n",
      "tensor([[8, 8, 8],\n",
      "        [8, 8, 8]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "fours_tensor = torch.full((2,3),4).to(torch_device)\n",
    "\n",
    "print(f'twos_tensor * fours_tensor:\\n{twos_tensor * fours_tensor}\\n')\n",
    "print(f'twos_tensor.mul(fours_tensor):\\n{twos_tensor.mul(fours_tensor)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('torch_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d94a055596b021af6b355d38c8f68b28eca02e4c507e6e85fc92b8f40d87732"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
