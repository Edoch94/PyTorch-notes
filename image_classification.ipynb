{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image classification using logistic regression in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Images\n",
    "The dataset used is [*MNIST Handwritten Digits Database*](http://yann.lecun.com/exdb/mnist/), consisting of 28px by 28px grayscale images of handwritten digits (0 to 9) and labels for each image indicating which digit it represents. \n",
    "\n",
    "Some sample images from the dataset:\n",
    "\n",
    "![mnist-sample](https://i.imgur.com/CAYnuo1.jpg)\n",
    "\n",
    "To work with images, the torchvision library is used. It contains some utilities for working with image data, and provides helper classes to download and import popular datasets, like MNIST, automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training dataset\n",
    "dataset = MNIST(\n",
    "    root='data/', \n",
    "    download=True,\n",
    "    train=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now download the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download test dataset\n",
    "test_dataset = MNIST(\n",
    "    root='data/', \n",
    "    download=True,\n",
    "    train=False\n",
    ")\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=L size=28x28>, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each element of dataset is a pair, consisting of a 28x28px image and a label. \n",
    "\n",
    "The image is an object of the class `PIL.Image.Image`, part of the Python imaging library [Pillow](https://pillow.readthedocs.io/en/stable/). It is possible to visualize the images using [`matplotlib`](https://matplotlib.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqgAAALICAYAAACzTAIGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAABG10lEQVR4nO3dd7idVZk3/nuRgBESahjEUPKTKjpIeyUUCVVEEJHiKL3pCAKjMOKIMAIBGcQCWOCNFBEYBDMUBYyAI02MSBFEKUZaQGqooRry/P4I4/iy1g77sM85e+1zPp/r4hrne9baz014snPzsO+9UtM0AQAAtZiv2wUAAMDf06ACAFAVDSoAAFXRoAIAUBUNKgAAVdGgAgBQFQ0qAABV0aB2KKV0QErpppTSKymlH3S7HpiXlNLiKaWLUkovpJQeSCnt3O2aoJWU0tUppZdTSrNe/+vubtcEJSmlt6WUTn/9ffX5lNKtKaWtul1XL9Ogdu4vEXFMRJzR7UKgDd+NiFcjYqmI2CUiTkkpvae7JcE8HdA0zejX/1ql28VACyMjYkZETIyIRSLiiIi4IKU0vptF9TINaoeaprmwaZqLI2Jmt2uBeUkpLRQRO0TEEU3TzGqa5vqI+ElE7NbdygB6W9M0LzRNc2TTNPc3TTOnaZpLI+K+iFi727X1Kg0qDB8rR8RrTdPc83fZbRHhCSo1Oy6l9GRK6VcppY27XQy0I6W0VMx9z/1Dt2vpVRpUGD5GR8Szb8iejYgxXagF2vHFiHhXRIyLiMkR8dOU0grdLQnmLaU0f0ScGxFnNU1zV7fr6VUaVBg+ZkXEwm/IFo6I57tQC7yppml+0zTN803TvNI0zVkR8auI+HC364JWUkrzRcTZMfez/gd0uZyepkGF4eOeiBiZUlrp77L3hf8ERe9oIiJ1uwgoSSmliDg95g6h7tA0zV+7XFJP06B2KKU0MqU0KiJGRMSIlNKolNLIbtcFb9Q0zQsRcWFEHJ1SWiiltEFEfDTm/ts+VCWltGhKacv/eU9NKe0SERtFxM+7XRu0cEpEvDsiPtI0zUvdLqbXaVA7d3hEvBQR/xYRu77+vw/vakXQ2v4R8faIeDwizouI/Zqm8QSVGs0fc7/C74mIeDIiDoyI7Zqm8V2oVCeltHxE/HNErBERj/7dd/fu0t3KeldqmqbbNQAAwN94ggoAQFU0qAAAVEWDCgBAVTSoAABUZZ5fh5RSMkHFgGiapt+/y9D9ykBxv9JL3K/0klb3qyeoAABURYMKAEBVNKgAAFRFgwoAQFU0qAAAVEWDCgBAVTSoAABURYMKAEBVNKgAAFRFgwoAQFU0qAAAVEWDCgBAVTSoAABURYMKAEBVNKgAAFRFgwoAQFU0qAAAVEWDCgBAVTSoAABURYMKAEBVNKgAAFRlZLcLqMWpp55azO+6664sO/HEEwe4GoarZZddNsu23nrrLLvpppuK+6dPn55lV111VZYttdRSxf3HHntsll122WXFtTNmzCjmAL1i9OjRWbbjjjsW1+69995tveYHPvCBYj5nzpz2C2vTDTfcUMx32mmnLHv00Uf7/foDyRNUAACqokEFAKAqGlQAAKqiQQUAoCqpaZrWP0yp9Q+HmFZDUqVfn/3222+gyxnymqZJ/f2avXS/fupTnyrmpftwXr9H3+hPf/pTlq288sodvWYrZ511VpZ961vfyrI77rij42t123C/XwfKl7/85SzbcMMNs6w0vBcRseSSS2bZnXfeWVy70UYb9bG6N9dqGGbVVVfNsieffDLLLrroouL+yZMnd1SX+7V9p5xySpbtu+++Hb3mfPOVn/29+uqrWfbwww8X177tbW/Lsne84x1tX2vq1KlZVhq4rUGr+9UTVAAAqqJBBQCgKhpUAACqokEFAKAqhqReN2nSpGJ+2GGHZdkmm2ySZddee22/1zSUDacP8W+22WZZ9tOf/rS4tvTB+E4HmlLKf6n7Y0iq9Lr33HNPlu21117F/dOmTeu4hsEynO7XgbD22msX8xtvvDHL+nK/DsTa0rpWa1sNqJRODCqtbXWy0IgRI4p5u9yv7fvv//7vLGs1/HbSSSdl2XPPPdf2tZ555pksO/nkk4trx48fn2VTpkzJsjXXXLO4/+mnn86yHXbYobj2mmuuKeaDxZAUAAA9QYMKAEBVNKgAAFRFgwoAQFU0qAAAVGVktwuoxcUXX1zMv/SlL7WVmeKnlcUXXzzLFlhggbb3X3bZZVl2+eWXF9c+8sgjWVY6TrG0LiLiRz/6Udt1HXzwwVm20korZdmVV15Z3F86grVVXQxN7X6bRF++dWIw17aawi+tffzxx7Nst912a/v6dK50VOgiiyySZffff39x/wknnJBljz32WMd1lZRq+Jd/+Zcsa9V7lP6+Jk6cWFzb7Sn+VjxBBQCgKhpUAACqokEFAKAqGlQAAKpiSOp1Dz74YDGfMWNGli2//PJZtuCCCxb3v/jii50VRs8bPXp0lrU6TrF0bN4XvvCFLCsdKdrKddddl2U33HBDcW3paN9Wjj766Cw74ogjsuzAAw8s7t92222z7P/+3//b9vXpHWeffXYxL/0+KGWlezii/P78xz/+sbh25syZ8yrxb1oNndx1111t7adeH/rQh7Js9dVXz7JWR58P1EBUu/bZZ5+uXn+weYIKAEBVNKgAAFRFgwoAQFU0qAAAVEWDCgBAVUzxv+6JJ54o5pMnT86yY445Jss+9rGPFfefe+65nRVGzygd3RkRcfzxx2dZqyMWzzvvvCzry8R+yeGHH55l888/f0evGRHx/PPPZ9mhhx6aZU8//XRx/3e/+90se/jhh4trL7300j5WR7esuuqqWbbKKqsU15Z+Hzz55JNZVjpWNyLilltu6WN1DGfTp0/Psvvuuy/LbrvttsEop8/23nvvLNtrr726UMng8AQVAICqaFABAKiKBhUAgKpoUAEAqIohqTdROt6udBSfISkWX3zxPuUlf/rTn/qrnL+5/vrr+/01+6I0aBgRcdBBB2XZSiutNNDl0I9KR0dedtllWdbqaN+S0v3Sav/aa6+dZXfeeWdxrWOnKb0XfuQjH8myu+++ezDK6bOxY8dm2Zw5c4prW+W9xBNUAACqokEFAKAqGlQAAKqiQQUAoCqp1Yk2EREppdY/HMamTJmSZdttt11x7ciR5tBKmqZpf2qiTd2+XydMmFDM+zKktNlmm2XZNddc85ZrqtlFF12UZaWhl4iIzTffPMs6PWGrL4bi/doXrYZATz311CxbYoklsqzVkFPpz5/S2lZ/TpXW/vGPfyyufeihh7Ls2GOPzbJuDxX2h+F+vw5VZ5xxRpbtsccexbVPPfVUlu24447Ftd3+M6bV/eoJKgAAVdGgAgBQFQ0qAABV0aACAFAVDSoAAFUxxf8W7Lrrrll21llnFdfut99+Wdbq6MfhZChOmbaaQC9NBS+wwALFtSNGjOjXmnpNq/ejG264Ics22GCDgS7nb4bi/doXn/70p4t5aYq/3cn8vqztyxR/p2tbTTqXvnWiVsP9fh0KSt8KM3Xq1CwbM2ZMcf99992XZSuuuGLnhQ0AU/wAAPQEDSoAAFXRoAIAUBUNKgAAVXEO51tw5513ZlmrD+avuuqqA10Olbj55puL+WWXXZZlrY7GHe7mzJnTp5zBceGFFxbzNddcs6PXLQ0etTpWtV2t9i+55JJZVnrf/uEPf1jc/3/+z//JsrvuuquP1UF7Ro0alWULLbRQFyrpHk9QAQCoigYVAICqaFABAKiKBhUAgKoYknoLHnzwwSybMWNGce2HPvShLDv88MOLa1988cXOCqNK559/fpa1GpJaeeWVs+yee+7p75J6ztixY7NsiSWWyLKZM2cORjnDzpNPPlnMSyfldeqKK67oaH+rmq655pos23DDDbOs1SBK6b3ckBSDab758meKpSwi4le/+tVAlzPgPEEFAKAqGlQAAKqiQQUAoCoaVAAAqqJBBQCgKqb434Innngiy6677rri2l122SXLWh1/esstt3RWGD1v6623zrKhOsW/9tprt712pZVWaiszxV+20UYbZVmr96Frr702y4bCtPpXv/rVLCsdQ9zKKqus0p/l0AXjx4/vaP9DDz2UZbNnz+7oNVspfXNJ6cjnqVOnFvf/y7/8S7/XNNg8QQUAoCoaVAAAqqJBBQCgKhpUAACqYkiqn1x00UXFfNddd82yJZdccqDLoSK33XZblj333HPFtWPGjBnocqpxxBFHZFmrY/tKwwG07+qrr86ypmmKa0uDZuuss05xbenY51qVjmtNKbWV0X0bb7xxMZ84cWLbr/Hv//7vWdaX95aTTz45y0rv5aX3/IiIiy++uO1rHX/88W2te+yxx4r5M8880/a1auUJKgAAVdGgAgBQFQ0qAABV0aACAFAVDSoAAFVJrSY5IyJSSq1/SFtee+21LGt1LGqrKcWhqGmafh+V7aX79YEHHijmCyywQJa9973vzbJeOtJzwoQJxfzKK6/MsgUXXLC49u67786yD3zgA1k2UL8uvX6/TpkyJcu222674trSFHvpmNCI8jcxdNvHPvaxYl76RpWPfvSjWdZqin/y5MlZtt9++/WxusHR6/frqFGjsux73/tece1uu+3W9uuWviVkIL4h5KWXXirmpWPSWxk3blyWjRgxIsve8Y53FPf30p8Rre5XT1ABAKiKBhUAgKpoUAEAqIoGFQCAqjjqdIB9//vfz7JPfepTxbWf/vSns6z0wXx6X6tBjt/+9rdZVrovjjvuuH6vqT+UBrrOPPPM4tq3v/3tbb/utGnTsqyXhgC6rd1jPlvlhx12WHFt6ejHvgyClCy//PJZVhqIa1XXKqusUlxb+vsqDQm3+nW5/vrrizn9rzT405dhqNNPP72Y//rXv86yeQ2Kv1X98Z7XrqH8PugJKgAAVdGgAgBQFQ0qAABV0aACAFAVQ1ID7KKLLsqyfffdtwuVUJNbbrml7bx0v/z+978v7r/00ks7K6wPJk6cmGWTJk3KspVWWqnja11++eUdv8Zw1un7UKtBkm984xtZ1pdhotKw4FprrZVlSyyxRHF/u4NPrZTWln6t5pXT/2bNmpVlrd7z3ve+92VZaRgqIuKss87qrLA2lU6simg9vFViKM8TVAAAKqNBBQCgKhpUAACqokEFAKAqGlQAAKqS5jXxmFLq/zPAiJtuuqmYL7fcclm21VZbZdnNN9/c7zUNtqZpyucJdmAo3K/bbLNNlp133nlZtuCCCxb3n3baaVlWmohtNe2/zjrrZFnp2MGIiIMPPjjL5syZU1zbrocffriYb7755ll2zz33dHStvuj1+3XJJZfMshtvvLG4dvz48VnW6p9raVq5tLbV8aHtHjXa6s+pTtfeeeedWbbDDjsU9991113FvEa9fr+WfOUrXynmhx9+eJa1+me1ySabZFnpGOC+GDt2bJZde+21xbWlbzR55plnimtL35JSOtb0sccee5MK69fqfvUEFQCAqmhQAQCoigYVAICqaFABAKiKIaku+O1vf1vMS0f8TZ48Ocv222+/fq9psA3FD/EPlBNPPDHLDjjggLb3d3oc5GC+7q677lrMf/SjH3X0up0aivfrl770pWJ+zDHHZFmnQ0o1DEkdd9xxbWUvvvhicX8vGYr3a2loKCLiv/7rv7JskUUWKa4tHZdaOv70pJNOKu7/3Oc+l2W77bZblq2xxhrF/U899VSW7bjjjsW111xzTTEfigxJAQDQEzSoAABURYMKAEBVNKgAAFRFgwoAQFVM8XfBxz72sWJ+4YUXZtmpp56aZab4y4bq/TpmzJgsO/roo4tr99133ywrHYs6mFP8Dz30UHF/qdarrrqq47oGwnC6XzfaaKMsazXxXzrmsVPXXXddlrU6urL0LScMr/t1s802y7If/OAHxbXvfOc7s+yvf/1rlrU6cnncuHFZNmLEiCxrddRp6RsyfvnLXxbXDiem+AEA6AkaVAAAqqJBBQCgKhpUAACqYkiqIn/4wx+yrDQE0OoYtl4ynD7EP5i23nrrLNt2222zbJ999mn7NW+55ZZiXhoEeOSRR7Ks1cDCzJkz266h29yv9JLhfr9usMEGxXz33XfPsr333ruja51++ulZ9pnPfKaj1xxuDEkBANATNKgAAFRFgwoAQFU0qAAAVMWQFF0x3D/ET29xv9JL3K/0EkNSAAD0BA0qAABV0aACAFAVDSoAAFXRoAIAUBUNKgAAVdGgAgBQFQ0qAABV0aACAFAVDSoAAFXRoAIAUBUNKgAAVdGgAgBQFQ0qAABV0aACAFAVDSoAAFXRoAIAUBUNKgAAVdGgAgBQFQ0qAABV0aACAFCV1DRNt2sAAIC/8QQVAICqaFABAKiKBhUAgKpoUAEAqIoGFQCAqmhQAQCoigYVAICqaFABAKiKBhUAgKpoUAEAqIoGFQCAqmhQO5RSeltK6fSU0gMppedTSremlLbqdl1QklK6OqX0ckpp1ut/3d3tmqDk7+7R//nrtZTSt7tdF7SSUhqfUro8pfR0SunRlNJ3Ukoju11Xr9Kgdm5kRMyIiIkRsUhEHBERF6SUxnezKJiHA5qmGf36X6t0uxgo+bt7dHRELBURL0XEj7tcFszL9yLi8YhYOiLWiLl9wf7dLKiXaVA71DTNC03THNk0zf1N08xpmubSiLgvItbudm0AQ8SOMfcP/uu6XQjMw/8XERc0TfNy0zSPRsTUiHhPl2vqWRrUfpZSWioiVo6IP3S7FmjhuJTSkymlX6WUNu52MdCGPSLih03TNN0uBObhpIj4REppwZTSuIjYKuY2qbwFGtR+lFKaPyLOjYizmqa5q9v1QMEXI+JdETEuIiZHxE9TSit0tyRoLaW0XMz9T6VndbsWeBPXxNwnps9FxEMRcVNEXNzNgnqZBrWfpJTmi4izI+LViDigy+VAUdM0v2ma5vmmaV5pmuasiPhVRHy423XBPOweEdc3TXNftwuBVl7vAX4eERdGxEIRMTYiFouI47tZVy/ToPaDlFKKiNNj7gf5d2ia5q9dLgna1URE6nYRMA+7h6en1G/xiFg2Ir7z+gOAmRFxZngA8JZpUPvHKRHx7oj4SNM0L3W7GChJKS2aUtoypTQqpTQypbRLRGwUc/+tH6qTUlo/5n4cxfQ+VWua5smYOyC93+vvr4vG3M9O39bVwnqYBrVDKaXlI+KfY+5XSjz6d9/Zt0t3K4PM/BFxTEQ8ERFPRsSBEbFd0zS+C5Va7RERFzZN83y3C4E2bB8RH4q577HTI2J2RHy+qxX1sGQoEgCAmniCCgBAVTSoAABURYMKAEBVNKgAAFRFgwoAQFVGzuuHKSUj/gyIpmn6/cvh3a8MFPcrvcT9Si9pdb96ggoAQFU0qAAAVEWDCgBAVTSoAABURYMKAEBVNKgAAFRFgwoAQFU0qAAAVEWDCgBAVTSoAABURYMKAEBVNKgAAFRFgwoAQFU0qAAAVGVktwsYaBtuuGGWXXfddVk2Z86cjq5z2mmnFfN//ud/7uh1AQCGG09QAQCoigYVAICqaFABAKiKBhUAgKpoUAEAqEpqmqb1D1Nq/cMuWnTRRbPs+9//fnHthAkTsuyd73xnlnU6xd/K0ksvnWVPPvnkgFyrlzRNk/r7NWu9X+l97ld6ifuVXtLqfvUEFQCAqmhQAQCoigYVAICqaFABAKhK1UNSe+65ZzHfbbfdsmyjjTZq+3Vvv/32LLvkkkva3n/QQQdl2SKLLFJce/bZZ2fZ3nvv3fa1hiof4h8Y888/f5aVjvvdYostivtTyv+xzOs9oh3/9E//VMzf9a53tf0as2fPzrKXX345y7bffvvi/iuvvLLta5W4XynZbLPNivkJJ5yQZWuttdZAl/M37tehaf3118+yqVOnFtf+6U9/yrJW748PPPBAZ4V1yJAUAAA9QYMKAEBVNKgAAFRFgwoAQFVGdruAedl9992L+Qc+8IEsmzZtWnHtrrvummWzZs3KspkzZ7Zd17rrrptlH/zgB9veDwPlM5/5TJaddNJJbe8fiCGpVkpDTk8//XRxbWkwccqUKf1eE7QyevToLLvggguKa++///4BroaBtOCCCxbz5ZZbLsu23XbbLFthhRWK+1dbbbWO6ho3blyWLbTQQsW1a6yxRltZRPeHpFrxBBUAgKpoUAEAqIoGFQCAqmhQAQCoigYVAICqVD3F3xff+973ivlATKeVjrFrNcVfOmZylVVWKa69++67OyuMYWPMmDHF/JBDDun3a7322mvF/L777mtr/4wZM4r5f/zHf2RZp0eSwkAZObL9Py6POuqoAayEt6p0FHTpPXOXXXYp7i9N4Ze++eSFF14o7r/11lvfrMR5XmuxxRZre/9Q4AkqAABV0aACAFAVDSoAAFXRoAIAUJWqh6Q23XTTbpdQ9Oijj2ZZq4GRlVZaKcu233774trjjjuus8IYkhZeeOEs++EPf1hcWzqKry/++te/Ztm///u/F9eWhpxgOLnooouK+U9+8pNBroR2lP6M/fznP9/2/jPPPDPLvvjFL2ZZq8HSZ555pu1rld733/3ud2fZhRdeWNz/jne8I8t+9rOftX39GniCCgBAVTSoAABURYMKAEBVNKgAAFRFgwoAQFVS0zStf5hS6x/y/zjjjDOK+R577JFlrY5+XGONNbKsL1N/vaRpmvx8uA4Nhfv1ve99b5aVJkfXXnvtAbl+6di+l19+ubi2ND36jW98I8tuueWWzgvrMvdr2VprrZVlK6ywQpYtssgixf1jx47NspVXXjnLll566eL+0jeqzOvPtDfaYostsuy8884rri1Na48YMaK4dvbs2W3XMBCG+/260047FfMLLrggyx5//PEs23LLLYv7f/e733VUV6dKR6pPnTq1uPa6667LsokTJ/Z7Tf2h1f3qCSoAAFXRoAIAUBUNKgAAVdGgAgBQlaqPOh2qxo0bV8wPOuigLDv66KMHuhy6oHSMXUTEpEmTsmygBqLa9ba3va2Yf/KTn8yybbfdNss+/elPF/e3GkahPu9617uK+fnnn59lK664Ypb1ZXCpL0pDfZ1ea4EFFijmpdft9jAUZa2GnEr/DPfZZ58s6/YwVCs77LBDlrW637/zne8MdDkDzhNUAACqokEFAKAqGlQAAKqiQQUAoCqGpPrJ3Xff3e0S6CHHHntsMf/oRz86yJX8v+64444sazUkVRqGWWihhbLsiCOOKO6/8sors+zJJ598sxIZYEsuuWSW3XjjjcW1iy22WJY9//zzWfbEE090XlhBaUhqzJgxxbWLL754lt1zzz1ZVjoNjaHrPe95T5ZdeumlXajk/1U6kW3nnXfOslYn9dXw99ApT1ABAKiKBhUAgKpoUAEAqIoGFQCAqmhQAQCoiin+fnL88ccX869+9auDXAm1GT9+fJbtueeeA3Kt//qv/8qyY445JsteeOGF4v6ZM2dm2SKLLFJce8UVV2RZabJ/1VVXLe6fMmVKlpWO8mtVFwPjs5/9bJaVpvUjIj73uc9l2SWXXJJlDz74YMd1lSy11FJZNnXq1OLa0hR/6X17xowZnRdGV/3gBz8o5nvvvXeWlY6Xfvrpp9t+3VdffbVPtb3R0ksvXcx/9rOfZdmCCy6YZYcddlhx/0svvdRRXTXwBBUAgKpoUAEAqIoGFQCAqmhQAQCoiiGpATbffP4dYLg75JBDsqx0JGhExMMPP5xlF154YZadfvrpxf1//OMfs2z27NlvVuI8tRoY+PjHP55lrY7dK9loo42ybNllly2uNSQ1eNZff/0smzZtWnHtd7/73SybM2dOv9fUSmnIafXVVy+uLdV6zjnntH2tkSPzPy7XWWed4tpWv14Mjuuvv76Yl95zfvzjH2fZqaeeWty/7rrrZtmhhx6aZc8++2xxf+n31mmnnVZcu9JKK2XZ97///SwrHRk9VOieAACoigYVAICqaFABAKiKBhUAgKqkpmla/zCl1j8cYkofgI+IWGaZZbKsdDJO6YPSERETJ07MslZDBA888ECWlU5gaTUg86tf/SrLHn300eLal19+uZgPlqZpUn+/Zq33a+mkkAMOOKC49v7778+y0gfja1AaACydDrXddtu1/ZrnnntuMd9tt93afo2BMJzu19LpTGussUZx7c9//vMBrmaufffdt5hPnjw5y26//fbi2k033TTLnnrqqbZrWHTRRbPs5ptvLq5dYYUV2n7dgTCc7tdOTZgwIctKp0tFlO+h0hDq73//++L+Nddcs+26jj322Cz7+te/nmXPPfdc269Zq1b3qyeoAABURYMKAEBVNKgAAFRFgwoAQFU0qAAAVMUU/+s+97nPFfMTTjiho9ctTToP1FGApWttsskmxbXXXnvtgNTQrl6fMt1yyy2zbK211iquPe644wa6nGr853/+Z5Z98pOf7Ph1S8f+TZ8+vePXbVev36+95P3vf3+WXXfddcW1f/7zn7OsdARvRMQdd9zRUV2lKf5WR/u+613v6uhanXK/dubtb397MT/xxBOz7FOf+lSWzauveqNvf/vbxbxVTzIUmeIHAKAnaFABAKiKBhUAgKpoUAEAqEr5fM8h7r777suysWPHdqGSgdfqA9gf/ehHs6x0zCZlpSNs99xzz+La4TQk9bWvfS3LPvGJT3ShEnrB6quvnmW//OUvs+zVV18t7v/sZz+bZZ0OQ/XFYB31yuBacMEFi/mHP/zhfr9WSv0+zzZkeIIKAEBVNKgAAFRFgwoAQFU0qAAAVEWDCgBAVYbMFP+qq65azA899NAsW2655bJsoI4f7dTpp5/e9trSkWurrbZace2UKVOyrHR858yZM9u+/nCyzTbbZNkiiyxSXLvhhhtm2fXXX9/vNUGtSseERkScf/75WTZq1Kgs+/KXv1zcX5r4HyjPP/98ln3hC18YtOszMEaOzNugiy66qLh23LhxWXbrrbdmWanviIj45je/mWUHHnhgce1zzz2XZUcccURx7VDlCSoAAFXRoAIAUBUNKgAAVdGgAgBQlSEzJLXjjjsW8z322CPL5puv/b78d7/7XZY99thjWVYaMOrrtY466qgsO/roo9vef84552TZBRdcUFy75pprZtno0aOzzJBU2T/+4z9mWdM0xbUrrrhilg3VIantt9++o/2//vWvi/mjjz7a0esyeEpDJyeffHJx7corr5xll156aZaVjtAdbK+99lqWzZo1qwuV0J9KvcMGG2xQXFt6j99iiy2y7KmnniruP+SQQ7Ls8ssvL67dfffds+zUU0/Nsocffri4fyjwBBUAgKpoUAEAqIoGFQCAqmhQAQCoypAZktprr72KebsnRLVad/XVV2fZb3/72ywrfVC6r9fqVGnwplR/RMT73//+LJs9e3Z/l0RELLvsst0uoSMLLLBAMd9qq62yrHSaWV98/etfL+aGUXrHWWedlWWf+MQnimt/85vfZNmee+6ZZbWe9Efv++xnP9v22tLwU6uBqJKrrroqy0477bTi2s985jNZtvzyy2eZISkAABgkGlQAAKqiQQUAoCoaVAAAqqJBBQCgKkNmiv+SSy4p5gceeGBHr3vQQQd1tL/bdt55526XMOx96UtfyrJHHnmkuLbVROdgWWWVVbJsnXXWKa49++yzO7rWueeem2VXXnllR6/J4Npkk02yrPSe02rS+fOf/3yWPf30050XBgNgm2226ffX/PnPf17MS1P8H/rQh7Lshhtu6PeaauEJKgAAVdGgAgBQFQ0qAABV0aACAFCV1DRN6x+m1PqHlRk1alQxLw2oHH744Vk2UEfpzTdf/u8ArQZB/umf/inLnn322X6vqQZN06T+fs3BvF9PP/30LGt13G7JSy+9VMzvueeeLPvzn/+cZRdccEFx/7333ptlzzzzTJattdZaxf3f/va3s2zJJZcsrm3XrbfeWswnTpyYZbUeadrr92unlllmmWJ+5513ZtnIkfns7cYbb1zcXzrqlM4N9/u1L6677ros22CDDYprS++FM2fObPta888/f5adccYZxbWlYcP11lsvy2688ca2r1+rVverJ6gAAFRFgwoAQFU0qAAAVEWDCgBAVTSoAABUZchM8ffFHnvskWWLL754ce3Xvva1LCtNcO+zzz7F/fvuu2+WTZ06tbj2scceK+ZDUa9PmS6xxBJZtueeexbXnnDCCQNczf96/vnns+yVV17JsrFjxxb3p5T/Y5nXe8Qb/fKXv8yyo446qrj22muvbft1u63X79dO/ehHPyrmO+20U5ZdccUVWbbVVlv1e020Ntzv174o/dk9efLk4tovfvGLWfb1r389y972trcV9++///5t7Y+I+MUvfpFlH/zgB4tre50pfgAAeoIGFQCAqmhQAQCoigYVAICqDMshKbpvKH6If8SIEcW8NDxV+rB9RMSKK67YnyX1WWlIqtXxo6Uj+s4888ws+93vftdxXd02FO/XVlZbbbUsa3VcbelY0zFjxmTZiy++2HlhtG043a+dWmCBBbLs+uuvL65dZZVVsqz0PrjtttsW948fP77ta2299dZZVutR0J0yJAUAQE/QoAIAUBUNKgAAVdGgAgBQFUNSdMVw/xD/8ssvX8w//elPd/S6n/nMZ7JsscUWy7Jf/epXxf2l051+/vOft712qBpO9+saa6yRZTfffHNx7Wc/+9ksO/XUU/u7JPpoON2vA2HZZZct5ldddVWWrbTSSllWOtEvIuLoo4/Osu9+97vFtS+//PK8ShxSDEkBANATNKgAAFRFgwoAQFU0qAAAVEWDCgBAVUzx0xWmTOkl7ld6ifuVXmKKHwCAnqBBBQCgKhpUAACqokEFAKAqGlQAAKqiQQUAoCoaVAAAqqJBBQCgKhpUAACqokEFAKAqGlQAAKqiQQUAoCoaVAAAqqJBBQCgKhpUAACqkpqm6XYNAADwN56gAgBQFQ0qAABV0aACAFAVDSoAAFXRoAIAUBUNKgAAVdGgAgBQFQ0qAABV0aACAFAVDSoAAFXRoAIAUBUNKgAAVdGg9pOU0koppZdTSud0uxaYl5TSJ1JKd6aUXkgp/Tml9IFu1wTz4v2VXpBSGp9Sujyl9HRK6dGU0ndSSiO7XVev0qD2n+9GxG+7XQTMS0ppi4g4PiL2iogxEbFRRNzb1aLgzXl/pRd8LyIej4ilI2KNiJgYEft3s6BepkHtBymlT0TEMxHxiy6XAm/mqIg4ummaaU3TzGma5uGmaR7udlHQivdXesj/FxEXNE3zctM0j0bE1Ih4T5dr6lka1A6llBaOiKMj4pBu1wLzklIaERHrRMSSKaXpKaWHXv9PUG/vdm1Q4v2VHnNSRHwipbRgSmlcRGwVc5tU3gINaucmRcTpTdPM6HYh8CaWioj5I2LHiPhAzP1PUGtGxOFdrAnmxfsrveSamPvE9LmIeCgiboqIi7tZUC/ToHYgpbRGRGweEd/qcinQjpde/7/fbprmkaZpnoyIb0bEh7tYExR5f6WXpJTmi4ifR8SFEbFQRIyNiMVi7mf+eQtMl3Vm44gYHxEPppQiIkZHxIiU0mpN06zVxbog0zTN0ymlhyKi6XYt0IaNw/srvWPxiFg2Ir7TNM0rEfFKSunMiDgmIg7tamU9KjWNP6veqpTSghGx8N9F/xpz31D3a5rmia4UBfOQUjo65n4uauuI+GtE/CQirm6a5oiuFgZv4P2VXpNSujciJkfE12Puv1CdGREvNk2zS1cL61H+E38HmqZ5sWmaR//nr4iYFREve/OkYpNi7tf13BMRd0bErRFxbFcrggLvr/Sg7SPiQxHxRERMj4jZEfH5rlbUwzxBBQCgKp6gAgBQFQ0qAABV0aACAFAVDSoAAFWZ5/egppRMUDEgmqZJ/f2a7lcGivuVXuJ+pZe0ul89QQUAoCoaVAAAqqJBBQCgKhpUAACqokEFAKAqGlQAAKqiQQUAoCoaVAAAqqJBBQCgKhpUAACqokEFAKAqGlQAAKqiQQUAoCoaVAAAqjKy2wUA9Vh88cWzbOrUqcW1p512WpZNnjy532sCYPjxBBUAgKpoUAEAqIoGFQCAqmhQAQCoigYVAICqpKZpWv8wpdY/hA40TZP6+zXdr+0bPXp0MX/22WezrNV7xPTp07Ns1VVX7aywSrlf6SXuV3pJq/vVE1QAAKqiQQUAoCoaVAAAqqJBBQCgKo467YIbbrihmK+//vqDXAnD1YEHHtjxazz11FP9UAlDTWlQ7oorriiu/chHPpJlt912W7/X1BeTJk0q5pdddlmWTZs2baDLoQeVjoyOiNhjjz2ybNNNNy2u3XLLLbNs5Mi8ZbvjjjuK+9ddd90se+mll4pra+UJKgAAVdGgAgBQFQ0qAABV0aACAFAVQ1JdsN566xXzCRMmZJkP4dOpJZdcMsu233774to5c+Zk2QknnFBcO3ny5M4Ko+eNGjUqy6ZOnZpl48aNK+4vDYgM5pDURhttlGX/+q//Wlx7//33Z5n3ZzbeeOMs++EPf1hcu8wyy2TZvE7zbGfte97znuLa0u+t0qBfzTxBBQCgKhpUAACqokEFAKAqGlQAAKqiQQUAoCo9OcX/+c9/PstK03EREYcccshAl9NvlltuuSwzJUqnLrzwwixbc801i2tfeOGFLDvssMP6vSaGhtLRi8suu2yWPfjgg8X9Z599dr/X1Bdf+MIXsmyBBRboQiX0gokTJ2ZZ6R5+5zvfWdxfen/9xS9+UVx7/vnnZ9nll1/+ZiX+zYsvvtj22lp5ggoAQFU0qAAAVEWDCgBAVTSoAABUpSeHpEoDUTvttFNx7YknnphlM2bM6O+SoAql3xvvfve7295/zjnn9Gc5DHHtDjmdeuqpxfzJJ5/sz3L6bOzYsV29PoNrt912y7I999wzy5555pni/g022CDLSkdJ//znPy/u33XXXbPsqaeeKq7FE1QAACqjQQUAoCoaVAAAqqJBBQCgKhpUAACq0pNT/KXjSw8++ODi2vXWWy/Luj3F3+3r0/tKx0lGRFxyySVZtuiii2bZnXfeWdx/9NFHd1QXw0vp/bXkxhtvHOBK3ppHH3202yUwiC677LIsKx01uskmmxT3b7fddlk2ZcqULNtnn32K+2fNmvUmFfL3PEEFAKAqGlQAAKqiQQUAoCoaVAAAqtKTQ1K9btq0ad0ugR43YcKEYr766qu3tX/SpEnF/LHHHnvLNTF0ffCDHyzmiy22WJbde++9WXbzzTf3e019VTrWdIklluhCJXRLu8eKvve97y3mKaUs+/Of/5xl/TEMtdBCC2XZzjvvnGUbbrhhcf8ee+zRcQ3d5gkqAABV0aACAFAVDSoAAFXRoAIAUBVDUhUZN25ct0ugR6y99tptr73tttuyrHTiFLTSapho5Mj8j5Brrrkmy5577rl+r6mvVlxxxSzbYIMNulAJtWuapu281dp2bb755sX81FNPzbLrrrsuy1qdojkUeIIKAEBVNKgAAFRFgwoAQFU0qAAAVEWDCgBAVUzxV+Tzn/98W9myyy5b3P/Nb34zyw455JDOC6OrjjzyyCzry+Tm8ccfn2WvvPJKce3GG2+cZeutt15xbemYy+222y7LzjzzzHkX+HfOOOOMYu4I1u4qHRMaEfHqq69m2QknnDDQ5cCAuvzyy4v5oYcemmWbbLJJlrX6/VI6YnrPPfcsrj355JOz7Mtf/nKWzZ49u7h/KPAEFQCAqmhQAQCoigYVAICqaFABAKiKIam3oDSktOOOOxbX7rTTTlnWauikU7/5zW8G5HUZPCNGjMiyLbbYoq11rZx33nltZRER882X/zvrnDlz2r5WyTHHHNP22ve9733F/BOf+ERHNdC+1VZbLcuOPvro4tq77767rayXvPbaa8X8wQcfHORK6Jbrr7++mF977bVZNnHixCy76667ivtHjx6dZaXB0oiIqVOnzqPC4cETVAAAqqJBBQCgKhpUAACqokEFAKAqhqReN2HChGJeGnLqyyk+nSpd6+GHHy6uveCCCwa6HAZYaSBq3XXXzbKmaTq6zgsvvFDMZ82alWWthqRKp62svfbaWbbGGmu0XVfp+gyugw46KMsWXnjh4tqf/vSnA11Ony244ILF/Ktf/Wpb+6dNm1bMr7zyyrdcE0ND6R4qDUmVTtmLKL9nGoZqzRNUAACqokEFAKAqGlQAAKqiQQUAoCoaVAAAqpLmNQ2cUupsVHiAfPzjH8+y888/v7j217/+dZYts8wyWVY6vrQ/lK7fykMPPZRlpb/XoaBpmtTfr1nr/Vqy/PLLF/PSUXrjxo3r6FrPPPNMlm299dbFtX05LnfVVVfNsssuuyzL+vL32urI4KeeeqrtugbCULxfWx2Xe9FFF2XZO9/5zuLajTbaKMtefPHFjupaYoklsmyRRRYprt11112z7PDDDy+unX/++bOs9Offhz70oeL+K664opjXaCjer4Np1KhRxbx0RPQmm2ySZSmVf/lL34ZR+paWiIgbb7xxXiUOKa3uV09QAQCoigYVAICqaFABAKiKBhUAgKoM+aNO11tvvX5/zdLxo1OmTCmunTFjRpa1OpK0NLzF0NTqKLx2B6JaDd+ddNJJWTZ9+vQs+93vftfWdSJa35fHHXdclpUGolodq1r6fdTtYajhZM011yzmpQG6H/3oR8W1pYGo0vBcq/u6dAzu/vvvn2Xjx48v7u+LVkf2vlFffm8wNJWG7yLK/cRee+2VZaNHjy7uP+uss7Ls61//enHtpptummWzZ88urh2qPEEFAKAqGlQAAKqiQQUAoCoaVAAAqqJBBQCgKj151GnJgw8+WMxLR5iWJut//OMfF/efeOKJbe3vi1bHl5aOa11uueX6/fo1GO5H8W277bbF/MILL2xr/0477VTMS8dU9sXKK6+cZVOnTi2uLd2bf/7zn7Ns9913L+7vy7Gq3TYU79cHHnigmJe+teHZZ58tri0dz1ya2F900UXbrmvWrFlZ1uoo63PPPTfLdt555+Lafffdt63rL7300sX88ccfb2t/DYbi/TpQ3vGOd2TZbbfdVlw7efLkLDviiCPavtbNN9+cZaVvsoiI+Id/+IcsmzlzZtvX6iWOOgUAoCdoUAEAqIoGFQCAqmhQAQCoypA56nSDDTYo5qWjyUrHRA7m4FGro05LgwCl+ofCkNRwt8suu7S99oknnsiyVh/ib9eRRx5ZzEvH9rU6pvL3v/99lh1//PFZ1kvDUEPVOuusk2Wl4ZBWFllkkWK+8MILZ1npfp02bVpx/6WXXpplpcHUl1566U0q/F8f/ehH2157++23Z1mro3kZmkpDnH/5y1+Ka4866qiOrnXddddlWashqQ9+8INZdt5553V0/V7jCSoAAFXRoAIAUBUNKgAAVdGgAgBQlSEzJNVqcKjXB4paDajQO8aMGZNlpdN6Wrnpppuy7N577y2uHT9+fJb98Ic/zLIJEyYU948YMSLLbr311uLaf/3Xf82yq6++uriW7tpjjz2ybOTI8tv/9OnTs+yUU04pri2dJDVlypQ+Vte/PvWpT7W99pvf/GaWGZIaXnbYYYcse+6554prZ8+e3dG17r777rbXbrrppllmSAoAALpIgwoAQFU0qAAAVEWDCgBAVTSoAABUZchM8Q8FP/7xj7OsdNTpt771rcEoh36y1FJLZdm6667b9v7SxH/pSNGIiN122y3L/uEf/qHta7344otZttlmmxXXPvvss22/Lt01adKkLDvnnHOKa0tT/DNnzuz3mvpD6VjTUaNGFdf+4Q9/yLKLLrqo32uit7z//e/PsiuuuGJArlU6cjilVFz73//93wNSQy/xBBUAgKpoUAEAqIoGFQCAqmhQAQCoiiGpivz617/OstKQFL3l/vvvz7JLL720uHabbbbJsn/8x39sK+uLVkdXloZpDEP1vscff7ytrNccdthhWTbffOXnLrNmzWorY3hpmibLXn311Y5ec/311y/mO+64Y5a1en8tDfUNN56gAgBQFQ0qAABV0aACAFAVDSoAAFXRoAIAUBVT/BV5+OGHs2zChAldqIT+NHv27Cz76le/WlxbmuLvi9J0fmky/4knnijuL020Qq3e/va3t732qquuGsBKGEpWWGGFYv7JT34yy9Zcc80sO/DAA4v7559//izbZ599imtvv/32eZU4LHiCCgBAVTSoAABURYMKAEBVNKgAAFTFkFRFHnzwwSxbdtllu1AJA+3GG28s5iNH+i0Jb7TkkksW8zFjxmRZq2Mqr7jiin6tiaFh+vTpWbbKKqsU155zzjltvebzzz9fzPfff/8sO+uss9p6zeHIE1QAAKqiQQUAoCoaVAAAqqJBBQCgKiYyKjJt2rQs+/GPf9yFSgDqsfvuuxfz5ZZbLstaDUNdf/31/VoTQ8Pee++dZccdd1xx7fjx47NsypQpWfatb32ruL80CE1rnqACAFAVDSoAAFXRoAIAUBUNKgAAVdGgAgBQldQ0TesfptT6h9CBpmlSf7+m+5WB4n6ll7hf6SWt7ldPUAEAqIoGFQCAqmhQAQCoigYVAICqaFABAKiKBhUAgKpoUAEAqIoGFQCAqmhQAQCoigYVAICqaFABAKiKBhUAgKpoUAEAqIoGFQCAqmhQAQCoSmqapts1AADA33iCCgBAVTSoAABURYMKAEBVNKgAAFRFgwoAQFU0qAAAVEWDCgBAVTSoAABURYMKAEBVNKgAAFRFgwoAQFU0qAAAVEWD2qGU0ttSSqenlB5IKT2fUro1pbRVt+uCkpTSOSmlR1JKz6WU7kkp7dvtmqCVlNIBKaWbUkqvpJR+0O16YF5SSounlC5KKb3wek+wc7dr6mUju13AEDAyImZExMSIeDAiPhwRF6SU/rFpmvu7WRgUHBcR+zRN80pKadWIuDqldGvTNDd3uzAo+EtEHBMRW0bE27tcC7yZ70bEqxGxVESsERGXpZRua5rmD12tqkd5gtqhpmleaJrmyKZp7m+aZk7TNJdGxH0RsXa3a4M3aprmD03TvPI//+/rf63QxZKgpaZpLmya5uKImNntWmBeUkoLRcQOEXFE0zSzmqa5PiJ+EhG7dbey3qVB7WcppaUiYuWI8G9MVCml9L2U0osRcVdEPBIRl3e5JIBet3JEvNY0zT1/l90WEe/pUj09T4Paj1JK80fEuRFxVtM0d3W7Hihpmmb/iBgTER+IiAsj4pV57wDgTYyOiGffkD0bc99reQs0qP0kpTRfRJwdcz9/ckCXy4F5aprmtdf/E9QyEbFft+sB6HGzImLhN2QLR8TzXahlSNCg9oOUUoqI02PuB6N3aJrmr10uCdo1MnwGFaBT90TEyJTSSn+XvS983O8t06D2j1Mi4t0R8ZGmaV7qdjFQklL6h5TSJ1JKo1NKI1JKW0bEJyPiv7tdG5SklEamlEZFxIiIGJFSGpVS8u0zVKdpmhdi7kemjk4pLZRS2iAiPhpz/8sqb0FqmqbbNfS0lNLyEXF/zP0c3+y/+9E/N01zbleKgoKU0pIRMSXm/lv9fBHxQESc3DTN97taGLSQUjoyIr7yhviopmmOHPxqYN5SSotHxBkRsUXM/eaJf2ua5j+7W1Xv0qACAFAV/4kfAICqaFABAKiKBhUAgKpoUAEAqMo8v64jpWSCigHRNE3q79d0vzJQ3K/0EvcrvaTV/eoJKgAAVdGgAgBQFQ0qAABV0aACAFAVDSoAAFXRoAIAUBUNKgAAVdGgAgBQFQ0qAABV0aACAFAVDSoAAFXRoAIAUBUNKgAAVdGgAgBQFQ0qAABV0aACAFAVDSoAAFXRoAIAUBUNKgAAVdGgAgBQFQ0qAABVGdntAnrR6NGjs2yPPfZoe//NN99czKdNm/aWawIgYvXVVy/mt912W5bNmTOn7dfddNNNs+yaa65pvzCqtPHGG2fZV77ylbbXHnXUUVl29dVXF/e3yinzBBUAgKpoUAEAqIoGFQCAqmhQAQCoiiGp140dO7aYn3vuuVn23ve+N8uWWmqptq/1wgsvFPOtt946y66//vq2X5fOjBkzppiXhuL64qmnnsqyV155pe39Sy65ZNv7n3vuufYLgyHoO9/5TjEvDUT1ZUhq/fXXzzJDUr2vL4NLpX/epYGqVkNWJaUhq4iII488su3XGKo8QQUAoCoaVAAAqqJBBQCgKhpUAACqkpqmaf3DlFr/sIeVToP48pe/XFy7ySab9Pv1U0rFvDRM8573vCfLHn/88X6vabA1TVP+RehAp/fr7bffXsxLQ3F9ceWVV2bZww8/3Pb+HXbYIcseeuih4trf/OY3WVb6YP8ll1xS3P/MM8+0XddwUuP9SsSee+6ZZV/72teKa5dYYoks68uQVGkQtvSeXQP36+D55S9/mWWlHqOvSsNbA9GP1KDV/eoJKgAAVdGgAgBQFQ0qAABV0aACAFAVDSoAAFUZ8lP8pWm60gTzQgst1NF1Zs+eXcxLE9Sbb755cW3pn8Vxxx2XZUcccUQfq6tPjVOmt956azGf1++R2pQmjd/5zndmWasjdLfbbrssmzlzZsd19boa79fhpNUxxN/4xjeybK+99iqunW++/HnMSSedlGXHHHNMcX/pGy768i0Ag8n92r5Sj9DqqNL+mM7vxFCd7DfFDwBAT9CgAgBQFQ0qAABV0aACAFCVkd0uYKAdfPDBWdbpQNS9996bZaeddlpxbenYvcmTJxfX7r333lk2bty4PlbHW7Xmmmt2u4SOrbDCCll27LHHZtnHP/7x4v79998/yyZNmtR5YdCmUaNGZVmrwaVWA1Htevnll7Os1uNLGRilo0o7VRpmiigPTbcayCopDWm1qn8oDE95ggoAQFU0qAAAVEWDCgBAVTSoAABURYMKAEBVhvxRp5tttlmWlSbrF1hggeL+J598Mst23333LJsxY0bbNa288srFvPSNA1tuuWWWbbjhhsX9Dz/8cNs1dJuj+AZP6d5s9e0Q06dPz7JW9+tw4n4dPGPHjs2yRx55pOPXLR11evzxx2fZYYcd1vG1um24369HHnlkMe/LxHy7StPyrab4+6I0nd+Xo1YHqq6B4KhTAAB6ggYVAICqaFABAKiKBhUAgKoM+aNOf/GLX2TZPvvsk2W/+93vBqGauVp94H/VVVfNsmWXXTbLRo8e3e81MTSU7u1lllkmy1566aXi/iOOOKLfa4K+WGSRRbpdAj1u4sSJHe1vNUw0mMeHlq7Vl8Gp0kBYrUNSrXiCCgBAVTSoAABURYMKAEBVNKgAAFRlyA9JlQzmQFTJAQccUMxLJ0Rde+21Wfb444/3e030lj322KOYl07GKTnuuOOK+fnnn/+Wa4L+8NOf/nRAXnfmzJlZdueddw7ItRg8nZ64dNRRR2VZq5Oouq00ONXqNNDSr0Hp16rV69bAE1QAAKqiQQUAoCoaVAAAqqJBBQCgKhpUAACqMiyn+Lttiy22KOa//vWvs2zbbbfNslmzZvV7TfSW+++/v5gvvPDCbe1fccUV+7EaeGt22mmnLBs7duyAXOuuu+7KsrPPPntArkX/azVZ35eJ/b68bq8ofQtBRPmo005/rQabJ6gAAFRFgwoAQFU0qAAAVEWDCgBAVYblkNT48eOz7JOf/GRx7c4775xlq622WkfXn2++8r8X3HDDDVmWUuroWgxN11xzTTFfbLHFsqw0BHDwwQcX92+//fZt11A6FrX0gf0ZM2a0/ZoMTdtss00xP/HEE7OsdA/3h/33339AXpfe0WqgqJe1GvIqDUn15TVqGB7zBBUAgKpoUAEAqIoGFQCAqmhQAQCoypAZklp11VWL+YEHHphlu+++e5a9/e1vb/taTdO0X1jBnDlzivm6666bZQ899FCWfetb3yruP+WUU7Lsscce62N19LIXXnghy77whS9kWathgdLvl1aDU3vuuWeW7bPPPll2xhlnFPcfeuihWTZz5sziWnrbUkstVczf8Y539Pu1br/99mJ+xx139Pu1GDwTJ05se22r97caBn8Gy9VXX51lTpICAIAOaFABAKiKBhUAgKpoUAEAqIoGFQCAqvTkFP9JJ52UZbvssktx7aKLLtrRtW655ZYsW2uttTp6zb5YaKGFsuzwww8vrt1ss82y7Ktf/Wpx7c9+9rPOCqOrRo8eXcxnzZrV1v5W64477ri2soiIbbfdNssmTZqUZXvttVdx/x//+McsKx19GRHx2muvFXPqs8QSS2TZFltsUVzb6htNOrHffvv1+2vSfb02gd5tpeOwe+3X0BNUAACqokEFAKAqGlQAAKqiQQUAoCpVD0m1GpgofQh+vvnKvfZf/vKXLPvP//zPLLv44ouL+0sfKu7LkNQf/vCHLPv4xz9eXLveeutl2TbbbJNlm2++edv7L7jgguLaL3/5y1l28sknF9fSXaVhpPXXX7+49t/+7d8Gupy/+clPfpJlpaHCn/70p8X9J5xwQpY988wzxbWnn35634qja8aOHZtlO+ywQxcqoVf12jAPA8MTVAAAqqJBBQCgKhpUAACqokEFAKAqGlQAAKpS9RT/gQceWMybpsmyz33uc8W1U6dOzbLp06dn2Zlnnlncv9tuu82jwv910UUXFfODDjooyx555JHi2rvvvjvLfvCDH2TZe9/73uL+gw8+OMve/e53F9cee+yxWbbAAgtk2Y9+9KPi/oceeqiY0/++8Y1vZNnll1/ehUreXOm+KH0LQUTEjTfemGVf/OIXi2uvuuqqLHvggQf6WB39bZlllsmywTxG+eijj86ym266adCuD7X6yle+0u0SOuYJKgAAVdGgAgBQFQ0qAABV0aACAFCVqoek+qLV0M7iiy+eZaWBqNKRoq3ce++9Wfbb3/62uLbVQFQn7rjjjmK+9957Z9nCCy9cXHvKKadk2X/8x39k2S677FLcv/XWW2dZ6VhZBkarwaMnn3wyyyZNmjTQ5czTjBkzivmsWbOybMUVVyyu/fSnP51lpeN6GVzf/va3s2zZZZft9+u0GogrDdrNnj2736/P4Lr66quz7KijjiquHQrDQJ0YysfCeoIKAEBVNKgAAFRFgwoAQFU0qAAAVCWVTmX62w9Tav3DQfDaa68V81LNf/zjH4trV1tttbau9dJLLxXz0qkku+++e5a1GgSp1ZgxY7Js8uTJWbbTTjsV95d+vVdfffW2r980TWp7cZu6fb8OlL322ivLSkNuEeXTwEoDByeccEJx/4svvphl11xzzZtU+L8mTJiQZeutt15xbek0s1GjRhXXloYgl1tuubbr6pT7tax0gl5fBk5LSsNz22+/fXHtL3/5y46uNVQNp/t1Xj3MG22yySZZVnp/7CV9+ftvpTSAduSRR3b8uu1qdb96ggoAQFU0qAAAVEWDCgBAVTSoAABURYMKAEBVqj7q9IorrijmW2yxRZa1mtYvTec/9dRTWdZqqvk73/nOvErsWc8//3yWlY6TXGaZZYr7W01m0/9KR/O+613vKq4tHf9ZmlxtdTzeK6+8kmV33XXXm1T4v0pHlS600EJt758+fXox/+53v9v2a9CZ+eefP8v222+/4tqBOGbx5ZdfzjLT+rRSmsJvdV+W7qNuT7C3UqphuB3r6gkqAABV0aACAFAVDSoAAFXRoAIAUJWqjzr97Gc/W8znzJnT9mvcd999WTZ16tS3XNNQNnr06Czbbbfd2t7f6vjNkuF0FN9AWHTRRYv52LFjs+ywww7LsnXXXbe4/93vfndHdZVccsklxbw0rDhp0qTi2vvvv78/S+qz4XS/fvGLX8yyY445ZtCuf/HFF2dZqyOXh5MNN9ywmF9//fVZNpzu15JWQ3UDMdQ3FKTU77dLnzjqFACAnqBBBQCgKhpUAACqokEFAKAqGlQAAKpS9RQ/Q9dwnzLttoUXXriYL7jggv1+rZkzZxbzv/71r/1+rYEynO7X1157Lcv68s0pnSp9k0SrI3CHqq222irLTj311OLa5ZdfPsuG0/3aF8Pp+NBaj3AtMcUPAEBP0KACAFAVDSoAAFXRoAIAUBVDUnSFD/HTS4bT/TphwoSuXv+2227LspdeeqkLlXRPafBp6aWXLq6dNm1alg2n+3UgtDoStd2jUidOnNj2/quvvrq9oqI8+NSX/bUyJAUAQE/QoAIAUBUNKgAAVdGgAgBQFUNSdIUP8dNL3K/0EvcrvcSQFAAAPUGDCgBAVTSoAABURYMKAEBVNKgAAFRFgwoAQFU0qAAAVEWDCgBAVTSoAABURYMKAEBVNKgAAFRFgwoAQFU0qAAAVEWDCgBAVTSoAABURYMKAEBVNKgAAFRFgwoAQFU0qAAAVEWDCgBAVTSoAABUJTVN0+0aAADgbzxBBQCgKhpUAACqokEFAKAqGlQAAKqiQQUAoCoaVAAAqvL/AwKdujH1M2KMAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "image, label = dataset[0]\n",
    "\n",
    "\n",
    "# plt.imshow(image, cmap='gray')\n",
    "# plt.title(f'label: {label}')\n",
    "\n",
    "figure = plt.figure(figsize=(10,10))\n",
    "cols, rows = 4, 4\n",
    "for pos in range(1,cols*rows+1):\n",
    "    id_sample = torch.randint(len(dataset),size=(1,)).item()\n",
    "    image, label = dataset[id_sample]\n",
    "    figure.add_subplot(rows, cols, pos)\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title(str(label))\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From images to tensors\n",
    "Images has to be converted to tensors so that PyTorch can work on them.\n",
    "\n",
    "This can be done using the `transforms` module of torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trasformation is performed during the import of the image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MNIST(\n",
    "    root='/data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 28, 28]), 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_tensor, label = dataset[0]\n",
    "img_tensor.shape, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image is now converted to a 1x28x28 tensor. \n",
    "- The first dimension tracks color channels. \n",
    "- The second and third dimensions represent pixels along the height and width of the image, respectively. \n",
    "\n",
    "Since images in the MNIST dataset are grayscale, there's just one channel. Other datasets have images with color, in which case there are three channels: red, green, and blue (RGB). \n",
    "\n",
    "The values in the image tensor range from 0 to 1, with `0` representing black, `1` white, and the values in between different shades of grey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation Datasets\n",
    "The dataset is split into\n",
    "1. **Training set** - used to train the model, i.e., compute the loss and adjust the model's weights using gradient descent\n",
    "2. **Validation set** - used to evaluate the model during training, adjust hyperparameters (learning rate, etc.), and pick the best version of the model\n",
    "3. **Test set** - used to compare different models or approaches and report the model's final accuracy\n",
    "\n",
    "In the MNIST dataset, there are 60,000 training images and 10,000 test images. The test set is standardized so that different researchers can report their models results against the same collection of images. \n",
    "\n",
    "There's no predefined validation set, so the training set must manually split the 60,000 images into training and validation datasets. This can be done using the `random_spilt` method, part of the `utils.data` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "train_ds, val_ds = random_split(dataset, [50000, 10000])\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader\n",
    "The dataset is divided into batches using the Dataloader method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
    "validation_loader = DataLoader(val_ds, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n",
    "\n",
    "The model used to classify the images is a logistic regression. A logistic regression model is almost identical to a linear regression model: it contains weights and bias matrices, and the output is obtained using simple matrix operations (`pred = x @ w.t() + b`).\n",
    "\n",
    "`nn.Linear` contains a built-in logistic regression model, that allows to automatically create the model and initialize the matrices.\n",
    "\n",
    "`nn.Linear` expects each training example to be a vector, so each `1x28x28` image tensor must be **flattened** into a vector (of size 784, `(28*28)`) before being passed into the model.\n",
    "\n",
    "The output for each image is a vector of size 10, with each element signifying the probability of a particular target label (i.e., 0 to 9). The predicted label for an image is the one with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Logistic regression model\n",
    "model = nn.Linear(\n",
    "    in_features=28*28,\n",
    "    out_features=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model parameter shapes are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: torch.Size([10, 784])\n",
      "biases: torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "print(f'weights: {model.weight.shape}')\n",
    "print(f'biases: {model.bias.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are 7850 parameters\n",
    "\n",
    "Check what's inside each element of the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 28, 28])\n",
      "tensor([8, 9, 2, 1, 1, 3, 7, 4, 0, 7, 4, 4, 4, 7, 6, 6, 0, 4, 6, 2, 7, 8, 0, 2,\n",
      "        1, 6, 7, 8, 1, 5, 0, 7, 3, 4, 5, 3, 3, 1, 8, 9, 8, 5, 0, 4, 5, 4, 0, 1,\n",
      "        6, 3, 0, 8, 9, 1, 7, 1, 7, 4, 3, 1, 8, 1, 4, 1, 9, 6, 9, 9, 3, 4, 6, 3,\n",
      "        9, 9, 9, 2, 8, 9, 7, 4, 9, 6, 5, 7, 3, 1, 8, 5, 3, 4, 5, 5, 6, 7, 0, 3,\n",
      "        0, 0, 5, 8, 9, 9, 0, 8, 8, 7, 4, 1, 4, 0, 1, 4, 8, 9, 3, 5, 8, 3, 4, 0,\n",
      "        9, 6, 9, 6, 8, 5, 5, 2])\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "image_0, label_0 = list(train_loader)[0]\n",
    "print(image_0.shape)\n",
    "print(label_0)\n",
    "print(len(label_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, each element of dataloader is a tuple of\n",
    "- a tensor of 4 dimensions:\n",
    "    - each element of the first dimension is a single image tensor (its dimension is 128 because there are 128 images in each batch)\n",
    "    - the other dimensions are for the color channel, the height, and the width (1, 28, 28)\n",
    "- a tensor containing the list of the image labels (one for each image tensor in the batch, so 128)\n",
    "\n",
    "The image tensors should be reshaped, such that each of them becomes a vector. In this way, each element (i.e., batch) of the dataloader becomes a matrix having the batch length as number of rows (128) and image $\\text{height}*\\text{width}$ as number of columns (784). To reshape the tensor the `.reshape()` method can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 784])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_0.reshape(-1,28*28).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and reshaping as a class\n",
    "Instead of manually performing the reshaping, it can be inserted in class together with the model using the `nn.Module` as a base. \n",
    "\n",
    "The `nn.Module` is a blueprint for PyTorch modules:\n",
    "- in the `__init__()` method the model is instantiated (and weights and biases are inizialised)\n",
    "- in the `forward()` method the operations on input data are implemented (included the linear model); it is invoked a batch of inputs is passed to the model\n",
    "\n",
    "So, in this case, the class is the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(28*28, 10)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        xb = xb.reshape(-1, 28*28)\n",
    "        out = self.linear(xb)\n",
    "        return out\n",
    "    \n",
    "model = MnistModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can be inspected using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=784, out_features=10, bias=True)\n",
      "\n",
      "[Parameter containing:\n",
      "tensor([[-0.0327, -0.0049,  0.0142,  ..., -0.0073,  0.0215,  0.0182],\n",
      "        [ 0.0034, -0.0024,  0.0356,  ..., -0.0018,  0.0172,  0.0059],\n",
      "        [ 0.0159, -0.0083, -0.0158,  ...,  0.0122, -0.0238,  0.0220],\n",
      "        ...,\n",
      "        [-0.0085,  0.0159, -0.0171,  ..., -0.0132, -0.0078, -0.0354],\n",
      "        [ 0.0224,  0.0290,  0.0289,  ..., -0.0304,  0.0107,  0.0241],\n",
      "        [-0.0010, -0.0225, -0.0026,  ...,  0.0066, -0.0044, -0.0014]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0280, -0.0296,  0.0297, -0.0270,  0.0241, -0.0154, -0.0020,  0.0006,\n",
      "         0.0343, -0.0003], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print(model.linear)\n",
    "print()\n",
    "print(list(model.linear.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model output\n",
    "The output is a vector of length 10, one element for each possible category. However, the output should be trasformed in a vector of probabilities, so that each element of the vector represents the probability that the image belongs to that category.\n",
    "\n",
    "To convert the output rows into probabilities the **softmax function** is used\n",
    "\n",
    "$$\\sigma(z_{j})=\\frac{e^{z_{j}}}{\\sum_{k=1}^{K}{e^{z_{j}}}}$$\n",
    "\n",
    "Where $z_{j}$ is the input value for class $j$.\n",
    "\n",
    "The softmax function is implemented in PyTorch in the `torch.nn.functional` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a single output of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 10])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_0 = model(image_0)\n",
    "\n",
    "output_0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to pass one of the rows of the output tensor (i.e., the vector containing the output values representing the \"probability\" of an image of belonging to each class) to the `softmax` function to transform it into an actual vector of probabilities.\n",
    "\n",
    "The `softmax` function has a `dim` parameter, where the user has to inserted the tensor dimension on which the function has to be applied\n",
    "\n",
    "See https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample probabilities:\n",
      " tensor([[0.0952, 0.0855, 0.1148, 0.1367, 0.0824, 0.1023, 0.0982, 0.1027, 0.0712,\n",
      "         0.1110],\n",
      "        [0.1180, 0.0869, 0.0951, 0.1440, 0.1024, 0.0640, 0.1158, 0.0931, 0.0857,\n",
      "         0.0948]])\n",
      "Sum:  2.0\n"
     ]
    }
   ],
   "source": [
    "# Apply softmax for each output row\n",
    "output_0_prob = softmax(output_0[:2], dim=1)\n",
    "\n",
    "# Look at sample probabilities\n",
    "print(\"Sample probabilities:\\n\", output_0_prob.data)\n",
    "\n",
    "# Add up the probabilities of an output row\n",
    "print(\"Sum: \", torch.sum(output_0_prob).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the predicted label (i.e., the label with the highest probability) the `torch.max()` function can be used\n",
    "\n",
    "This function returns a namedtuple (values, indices) where values is the maximum value of each row of the input tensor in the given dimension dim. And indices is the index location of each maximum value found (argmax).\n",
    "\n",
    "See https://pytorch.org/docs/stable/generated/torch.max.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 3])\n",
      "tensor([0.1367, 0.1440], grad_fn=<MaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "max_probs, preds = torch.max(output_0_prob, dim=1)\n",
    "print(preds)\n",
    "print(max_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metric and Loss Function\n",
    "To evaluated the results of a classification the accuracy metric can be used. The accuracy can be implemented as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(torch.eq(preds,labels)).item() / len(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in the previous example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 10])\n",
      "torch.Size([128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0781)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_0, label_0 = list(train_loader)[0]\n",
    "output_0 = model(image_0)\n",
    "\n",
    "print(output_0.shape)\n",
    "print(label_0.shape)\n",
    "\n",
    "accuracy(output_0, label_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy can't be used as a loss function for optimizing to model using gradient descent for the following reasons:\n",
    "\n",
    "1. It's not a differentiable function. `torch.max` and `==` are both non-continuous and non-differentiable operations\n",
    "\n",
    "2. It doesn't take into account the actual probabilities predicted by the model, so it can't provide sufficient feedback for incremental improvements.\n",
    "\n",
    "For these reasons, accuracy is used as an **evaluation metric** for classification, but not as a loss function. A commonly used loss function for classification problems is the **cross-entropy**, which has the following formula:\n",
    "\n",
    "$$H(\\hat{p},y)=-\\sum_{j}{y_{j}\\log{\\hat{p}_{j}}}$$\n",
    "\n",
    "Where \n",
    "- $\\hat{p}$ is the vector of the predicted probabilities of the correct labels\n",
    "- $y$ is the vector of the correct labels (one-hot encoded)\n",
    "\n",
    "All in all, what the cross-entropy does is summing the predicted log-probabilities of the correct labels across all the observations\n",
    "\n",
    "Unlike accuracy, cross-entropy is a continuous and differentiable function. It also provides useful feedback for incremental improvements in the model (a slightly higher probability for the correct label leads to a lower loss). These two factors make cross-entropy a better choice for the loss function.\n",
    "\n",
    "PyTorch provides an efficient and tensor-friendly implementation of cross-entropy as part of the `torch.nn.functional` package. Moreover, it also performs softmax internally, so the model outputs can be passed directly without converting them into probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Manually implemented cross-entrompy loss for binary classification)\n",
    "Source: https://machinelearningmastery.com/cross-entropy-for-machine-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected: [0.0, 1]\tpredicted: [0.19999999999999996, 0.8]\t[y=1.0, yhat=0.8] ce: 0.223 nats\n",
      "expected: [0.0, 1]\tpredicted: [0.09999999999999998, 0.9]\t[y=1.0, yhat=0.9] ce: 0.105 nats\n",
      "expected: [0.0, 1]\tpredicted: [0.09999999999999998, 0.9]\t[y=1.0, yhat=0.9] ce: 0.105 nats\n",
      "expected: [0.0, 1]\tpredicted: [0.4, 0.6]\t[y=1.0, yhat=0.6] ce: 0.511 nats\n",
      "expected: [0.0, 1]\tpredicted: [0.19999999999999996, 0.8]\t[y=1.0, yhat=0.8] ce: 0.223 nats\n",
      "expected: [1.0, 0]\tpredicted: [0.9, 0.1]\t[y=0.0, yhat=0.1] ce: 0.105 nats\n",
      "expected: [1.0, 0]\tpredicted: [0.6, 0.4]\t[y=0.0, yhat=0.4] ce: 0.511 nats\n",
      "expected: [1.0, 0]\tpredicted: [0.8, 0.2]\t[y=0.0, yhat=0.2] ce: 0.223 nats\n",
      "expected: [1.0, 0]\tpredicted: [0.9, 0.1]\t[y=0.0, yhat=0.1] ce: 0.105 nats\n",
      "expected: [1.0, 0]\tpredicted: [0.7, 0.3]\t[y=0.0, yhat=0.3] ce: 0.357 nats\n",
      "Average Cross Entropy: 0.247 nats\n"
     ]
    }
   ],
   "source": [
    "# calculate cross entropy for classification problem\n",
    "from math import log\n",
    "from numpy import mean\n",
    " \n",
    "# calculate cross entropy\n",
    "def cross_entropy(p, q):\n",
    "    return -sum([p[i]*log(q[i]) for i in range(len(p))])\n",
    " \n",
    "# define classification data\n",
    "p = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
    "q = [0.8, 0.9, 0.9, 0.6, 0.8, 0.1, 0.4, 0.2, 0.1, 0.3]\n",
    "# calculate cross entropy for each example\n",
    "results = list()\n",
    "for i in range(len(p)):\n",
    "    # create the distribution for each event {0, 1}\n",
    "    expected = [1.0 - p[i], p[i]]\n",
    "    predicted = [1.0 - q[i], q[i]]\n",
    "    # calculate cross entropy for the two events\n",
    "    ce = cross_entropy(expected, predicted)\n",
    "    print(f'expected: {expected}\\tpredicted: {predicted}\\t[y={p[i]:.1f}, yhat={q[i]:.1f}] ce: {ce:.3f} nats')\n",
    "    # print(f'[y={p[i]:.1f}, yhat={q[i]:.1f}] ce: {ce:.3f} nats')\n",
    "    results.append(ce)\n",
    "\n",
    "# calculate the average cross entropy\n",
    "mean_ce = mean(results)\n",
    "print(f'Average Cross Entropy: {mean_ce:.3f} nats')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Having defined data loaders, model, loss function and optimizer, the model can be trained. The training process is identical to linear regression, with the addition of a \"validation phase\" to evaluate the model in each epoch.\n",
    "\n",
    "The pseudocode is:\n",
    "\n",
    "```\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    for batch in train_loader:\n",
    "        # Generate predictions\n",
    "        # Calculate loss\n",
    "        # Compute gradients\n",
    "        # Update weights\n",
    "        # Reset gradients\n",
    "    \n",
    "    # Validation phase\n",
    "    for batch in val_loader:\n",
    "        # Generate predictions\n",
    "        # Calculate loss\n",
    "        # Calculate metrics (accuracy etc.)\n",
    "    # Calculate average validation loss & metrics\n",
    "    \n",
    "    # Log epoch, loss & metrics for inspection\n",
    "```\n",
    "\n",
    "Some parts of the training loop are specific the specific problem (e.g. loss function, metrics etc.) whereas others are generic and can be applied to any problem. \n",
    "\n",
    "The problem-independent parts are in included within a function called `fit`, which is used to train the model. The problem-specific parts are implemented by adding new methods to the `nn.Module` class.\n",
    "\n",
    "Starting from `nn.Module`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(28*28, 10)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        xb = xb.reshape(-1, 784)\n",
    "        out = self.linear(xb)\n",
    "        return out\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = cross_entropy(out, labels)   # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                      # Generate predictions\n",
    "        loss = cross_entropy(out, labels)       # Calculate loss\n",
    "        acc = accuracy(out, labels)             # Calculate accuracy\n",
    "        return {'val_loss': loss, 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))\n",
    "    \n",
    "model = MnistModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model a function is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader):\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    history = [] # for recording epoch-wise results\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Training Phase \n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "\n",
    "    return history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('torch_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d94a055596b021af6b355d38c8f68b28eca02e4c507e6e85fc92b8f40d87732"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
